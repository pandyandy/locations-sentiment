import streamlit as st
import pandas as pd
import numpy as np
import networkx as nx
import matplotlib.pyplot as plt
import plotly.express as px

from wordcloud import WordCloud
from scripts.viz import sentiment_color

def create_network_graph(attributes, slider_entities):
    # Get top entities by total attribute counts
    pivot_attrs = attributes.pivot(index='ENTITY', columns='ATTRIBUTE', values='COUNT').fillna(0)
    pivot_attrs['Total'] = pivot_attrs.sum(axis=1)
    top_entities = pivot_attrs.nlargest(slider_entities, 'Total').index.tolist()
    
    # Initialize graph and figure
    G = nx.Graph()
    fig, ax = plt.subplots(figsize=(15, 10))
    
    # Calculate entity positions in a circle
    entity_positions = calculate_entity_positions(top_entities)
    
    # Add nodes and edges to graph
    add_nodes_and_edges(G, top_entities, attributes)
    
    # Position attribute nodes
    pos = position_attribute_nodes(G, entity_positions)
    
    # Draw the network
    draw_network(G, pos, top_entities)
    
    # Configure plot
    ax.axis('off')
    ax.set_xlim(-2, 2)
    ax.set_ylim(-2, 2)
    
    return fig

def calculate_entity_positions(entities, radius=1.5):
    return {entity: (radius * np.cos(2 * np.pi * i / len(entities)), radius * np.sin(2 * np.pi * i / len(entities))) 
            for i, entity in enumerate(entities)}

def add_nodes_and_edges(G, top_entities, attributes):
    for entity in top_entities:
        G.add_node(entity, node_type='entity')
        entity_attrs = attributes[attributes['ENTITY'] == entity]
        
        for _, row in entity_attrs.iterrows():
            attr, count = row['ATTRIBUTE'], row['COUNT']
            G.add_node(attr, node_type='attribute') if attr not in G else None
            G.add_edge(entity, attr, weight=count)

def position_attribute_nodes(G, entity_positions, scale_factor=0.8):
    pos = entity_positions.copy()
    attr_nodes = [n for n in G.nodes() if n not in entity_positions]
    attr_connections = {attr: len([n for n in G.neighbors(attr) if n in entity_positions]) for attr in attr_nodes}
    attr_nodes.sort(key=lambda x: attr_connections[x], reverse=True)
    
    occupied_positions = []
    min_distance = 0.2

    for attr in attr_nodes:
        connected_entities = [n for n in G.neighbors(attr) if n in entity_positions]
        if connected_entities:
            attempts = 0
            while attempts < 50:
                if attr_connections[attr] > 1:
                    x, y = np.mean([entity_positions[e] for e in connected_entities], axis=0) * scale_factor
                    offset = 0.15 + (0.1 * attempts / 50)
                    angle = np.random.uniform(0, 2 * np.pi)
                    x += offset * np.cos(angle)
                    y += offset * np.sin(angle)
                else:
                    entity = connected_entities[0]
                    angle = 2 * np.pi * attempts / 50
                    radius = 0.25 + (0.1 * attempts / 50)
                    x = entity_positions[entity][0] + radius * np.cos(angle)
                    y = entity_positions[entity][1] + radius * np.sin(angle)

                position = (x, y)
                if all(np.sqrt((x - ox)**2 + (y - oy)**2) > min_distance for ox, oy in occupied_positions):
                    occupied_positions.append(position)
                    pos[attr] = position
                    break
                
                attempts += 1
            
            if attr not in pos:
                pos[attr] = position
                occupied_positions.append(position)

    return pos

def draw_network(G, pos, top_entities):
    attr_nodes = [n for n in G.nodes() if n not in top_entities]
    nx.draw_networkx_nodes(G, pos, nodelist=top_entities, node_color='#e6f2ff', node_size=2500)
    nx.draw_networkx_nodes(G, pos, nodelist=attr_nodes, node_color='#F2F2F2', node_size=1000, alpha=0.8)
    
    colors = plt.cm.rainbow(np.linspace(0, 1, len(top_entities)))
    for i, entity in enumerate(top_entities):
        entity_edges = [(u, v) for (u, v) in G.edges() if u == entity or v == entity]
        if entity_edges:
            edge_weights = [G[u][v]['weight'] for u, v in entity_edges]
            nx.draw_networkx_edges(G, pos, edgelist=entity_edges, 
                                     width=[w / max(edge_weights) * 2 for w in edge_weights],
                                     edge_color=[colors[i]], alpha=0.7)
    
    nx.draw_networkx_labels(G, pos, labels={node: node for node in top_entities}, font_size=10, font_color='#238dff', font_weight='600')
    nx.draw_networkx_labels(G, pos, labels={node: node for node in attr_nodes}, font_size=8)

@st.fragment
def display_network_graph(attributes):
    col1, col2 = st.columns([0.9, 0.1], vertical_alignment='center')
    unique_entities_count = attributes['ENTITY'].nunique()
    max_value = unique_entities_count if unique_entities_count < 10 else 10
    col1.caption(f"_See up to top {max_value} mentioned entities and their attributes. The wider the line, the more times the attribute was mentioned._")
    num_entities = col2.number_input("Select the number of entities", min_value=1, max_value=max_value, value=max_value if max_value < 5 else 5)
    
    fig = create_network_graph(attributes, num_entities)
    col1.pyplot(fig, use_container_width=True)


def ai_analysis(data, attributes):
    ## SENTIMENT COUNT BY DATE
    data['REVIEW_DATE'] = pd.to_datetime(data['REVIEW_DATE']).dt.date
    avg_rating_per_day = data.groupby('REVIEW_DATE')['RATING'].mean().reset_index()
    color_scale = avg_rating_per_day['RATING'].apply(lambda x: '#EA4335' if x < 1.5 else '#e98f41' if x < 2.5 else '#FBBC05' if x < 3.6 else '#a5c553' if x < 4.5 else '#34A853').tolist()

    fig_avg_rating_per_day = px.line(
        avg_rating_per_day,
        x='REVIEW_DATE',
        y='RATING',
        labels={'RATING': 'Average Rating', 'REVIEW_DATE': 'Date'},
        title='Average Rating per Date',
        height=300
    )
    fig_avg_rating_per_day.update_traces(mode='lines+markers', hovertemplate='Avg Rating: %{y:.2f}<extra></extra>', line=dict(color='#E6E6E6'), marker=dict(color=color_scale))  
    fig_avg_rating_per_day.update_layout(xaxis_title=None, yaxis_title=None, hovermode='x')
    st.plotly_chart(fig_avg_rating_per_day, use_container_width=True)

    ## ENTITY-ATTRIBUTE RELATIONS
    st.divider()
    st.markdown("##### Entity-Attribute Relations")

    filtered_reviews_ids = data['REVIEW_ID'].unique()

    attributes = attributes[attributes['REVIEW_ID'].isin(filtered_reviews_ids)]
    attributes = attributes.dropna(subset=['ENTITY'])
    attributes = attributes[attributes['ATTRIBUTE'].notna() & (attributes['ATTRIBUTE'] != '')]

    attributes['ENTITY'] = attributes['ENTITY'].apply(lambda x: 'pan칤' if 'pan칤' in x and len(x.split()) > 1 else x)
    attributes['ENTITY'] = attributes['ENTITY'].apply(lambda x: 'pobo캜ka' if 'pobo캜ka' in x and len(x.split()) > 1 else x)
    attributes['ENTITY'] = attributes['ENTITY'].replace({'pristup': 'p콏칤stup', 'pani': 'pan칤'})
    attributes.loc[(attributes['ENTITY'] == 'pan칤') & (attributes['ATTRIBUTE'] == 'pomocn칳'), 'ATTRIBUTE'] = 'pomocn치'
    attributes = attributes[~((attributes['ENTITY'] == 'poji코콘ovna') & (attributes['ATTRIBUTE'] == 'Dv콢r Kr치lov칠 nad Labem'))]

    attributes = attributes.groupby(['ENTITY', 'ATTRIBUTE']).size().reset_index(name='COUNT')
    attributes = attributes[attributes['COUNT'] > 0]

    attributes_sorted = attributes.sort_values(['ENTITY', 'COUNT'], ascending=[True, False])  # Sort by ENTITY and COUNT
    attributes_limited = attributes_sorted.groupby('ENTITY').head(8)

    if attributes_limited is not None and not attributes_limited.empty:
        display_network_graph(attributes_limited)
    else: 
        st.info('No entity-attribute relations found for the selected filters.', icon=':material/info:')

    ## REVIEW DETAILS
    st.markdown("##### Review Details")

    if data.empty:
        st.info("No reviews available for the selected filters.", icon=':material/info:')
        st.stop()

    columns = ['REVIEW_DATE', 'REVIEW_TEXT', 'RATING', 'SENTIMENT', 'ADDRESS', 'CATEGORY', 'REVIEWER_NAME', 'REVIEW_URL']
    st.dataframe(data[columns].style.map(sentiment_color, subset=["SENTIMENT"]),
                column_config={
                    'REVIEW_DATE': 'Date',
                    'RATING': 'Rating',
                    'REVIEW_TEXT': st.column_config.Column(
                        'Review',
                        width="large"),
                    'SENTIMENT': 'Sentiment',
                    'ADDRESS': st.column_config.Column(
                        'Location',
                        width="small"),
                    'CATEGORY': st.column_config.Column(
                        'Category',
                        width="medium"),
                    'REVIEWER_NAME': 'Author',
                    'REVIEW_URL': st.column_config.LinkColumn(
                        '游댕',
                        width='small',
                        help='Link to the review',
                        display_text='URL')
                },
                column_order=columns,
                hide_index=True, 
                use_container_width=True)


    st.markdown("##### Keywords")
    # Import the WordCloud class

    stop_words = set(["a","aby","ahoj","aj","ale","anebo","ani","ani","ano","asi","aspo켿","atd","atp","az","a캜koli","a","bez","beze","bl칤zko","bohu쬰l","brzo","bude","budem","budeme","budes","budete","bude코","budou","budu","by","byl","byla","byli","bylo","byly","bys","byt","b칳t","b캩hem","chce","chceme","chcete","chce코","chci","cht칤t","cht캩j칤","chut'","chuti","ci","clanek","clanku","clanky","co","coz","co","cz","daleko","dalsi","dal코칤","den","deset","design","devaten치ct","dev캩t","dnes","do","dobr칳","docela","dva","dvacet","dvan치ct","dv캩","d치l","d치le","d캩kovat","d캩kujeme","d캩kuji","email","ho","hodn캩","i","jak","jakmile","jako","jako","jde","je","jeden","jeden치ct","jedna","jedno","jednou","jedou","jeho","jeho","jej","jeji","jejich","jej칤","jeliko","jemu","jen","jenom","jen","jeste","jestli","jestli쬰","je코t캩","je","ji","jich","jimi","jinak","jine","jin칠","jiz","ji","jsem","jses","jse코","jsi","jsme","jsou","jste","j치","j칤","j칤m","j칤","j코te","k","kam","ka쬯칳","kde","kdo","kdy","kdyz","kdy","ke","kolik","krom캩","ktera","ktere","kteri","kterou","ktery","kter치","kter칠","kter칳","kte콏i","kte콏칤","ku","kv콢li","ma","maj칤","mate","me","mezi","mi","mit","mne","mnou","mn캩","moc","mohl","mohou","moje","moji","mo쬹치","muj","mus칤","muze","my","m치","m치lo","m치m","m치me","m치te","m치코","m칠","m칤","m칤t","m캩","m콢j","m콢쬰","na","nad","nade","nam","napiste","napi코te","naproti","nas","nasi","na캜e","na코e","na코i","ne","nebo","nebyl","nebyla","nebyli","nebyly","nech콘","ned캩laj칤","ned캩l치","ned캩l치m","ned캩l치me","ned캩l치te","ned캩l치코","neg","nejsi","nejsou","nemaj칤","nem치me","nem치te","nem캩l","neni","nen칤","nesta캜칤","nevad칤","nez","ne","nic","nich","nimi","nove","novy","nov칠","nov칳","nula","n치","n치m","n치mi","n치s","n치코","n칤","n칤m","n캩","n캩co","n캩jak","n캩kde","n캩kdo","n캩mu","n캩mu","o","od","ode","on","ona","oni","ono","ony","osm","osmn치ct","pak","patn치ct","po","pod","podle","pokud","potom","pouze","pozd캩","po콏치d","prave","prav칠","pred","pres","pri","pro","proc","prost캩","pros칤m","proti","proto","protoze","proto쬰","pro캜","prvni","prvn칤","pr치ve","pta","p캩t","p콏ed","p콏ede","p콏es","p콏ese","p콏i","p콏i캜em","re","rovn캩","s","se","sedm","sedmn치ct","si","sice","skoro","sm칤","sm캩j칤","snad","spolu","sta","sto","strana","st칠","sve","svych","svym","svymi","sv칠","sv칳ch","sv칳m","sv칳mi","sv콢j","ta","tady","tak","take","takhle","taky","takze","tak칠","tak쬰","tam","tamhle","tamhleto","tamto","tato","te","tebe","tebou","ted'","tedy","tema","ten","tento","teto","ti","tim","timto","tipy","tis칤c","tis칤ce","to","tob캩","tohle","toho","tohoto","tom","tomto","tomu","tomuto","toto","tro코ku","tu","tuto","tvoje","tv치","tv칠","tv콢j","ty","tyto","t칠ma","t칠to","t칤m","t칤mto","t캩","t캩m","t캩ma","t캩mu","t콏eba","t콏i","t콏in치ct","u","ur캜it캩","uz","u","v","vam","vas","vase","va코e","va코i","ve","vedle","ve캜er","vice","vlastn캩","vsak","vy","v치m","v치mi","v치s","v치코","v칤ce","v코ak","v코echen","v코echno","v코ichni","v콢bec","v쬯y","z","za","zat칤mco","za캜","zda","zde","ze","zpet","zpravy","zpr치vy","zp캩t","캜au","캜i","캜l치nek","캜l치nku","캜l치nky","캜trn치ct","캜ty콏i","코est","코estn치ct","쬰"])
    keywords_text = " ".join(word for word in data['KEYWORDS'].dropna().astype(str).values if word not in stop_words).replace("'", "")
    if keywords_text:  # Skip if there are no words in keywords_text
        wordcloud = WordCloud(width=2500, height=500, background_color='white', colormap='CMRmap_r').generate(keywords_text)
        # Create new figure and axis
        fig, ax = plt.subplots(figsize=(10, 10))
        ax.imshow(wordcloud, interpolation='bilinear')
        ax.axis("off")
        
        st.pyplot(fig)
    else:
        st.info("No keywords available for the selected filters.", icon=':material/info:')
        st.stop()